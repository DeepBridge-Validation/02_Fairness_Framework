# Data Directory

This directory contains all datasets used in the experiments.

## ğŸ“ Structure

```
data/
â”œâ”€â”€ raw/                    # Raw, unprocessed datasets
â”œâ”€â”€ processed/              # Preprocessed datasets ready for analysis
â”œâ”€â”€ synthetic/              # 500 synthetic datasets for testing (gitignored)
â”œâ”€â”€ case_studies/           # Real-world case study datasets
â”œâ”€â”€ ground_truth/           # Manual annotations and ground truth
â””â”€â”€ datasets_metadata.csv   # Metadata for all synthetic datasets
```

## ğŸ“Š Dataset Overview

### Synthetic Datasets
- **Location**: `synthetic/`
- **Count**: 500 CSV files
- **Format**: `synthetic_XXXX.csv`
- **Purpose**: Automated bias detection validation (Experiment 1)
- **Generation**: See `scripts/generate_synthetic_data.py`
- **Metadata**: `datasets_metadata.csv` contains all parameters

**Note**: Synthetic datasets are large and gitignored. To generate them:
```bash
python scripts/generate_synthetic_data.py --count 500 --output data/synthetic/
```

### Case Studies
- **Location**: `case_studies/`
- **Datasets**:
  1. **COMPAS**: Recidivism prediction (ProPublica)
  2. **Adult**: Income prediction (UCI ML Repository)
  3. **Bank**: Marketing campaign (UCI ML Repository)
  4. **German Credit**: Credit risk assessment (UCI ML Repository)
- **Purpose**: Real-world validation (Experiment 4)

### Ground Truth
- **Location**: `ground_truth/`
- **Files**:
  - `ground_truth_final.json`: Final consensus annotations
  - `annotations_annotator_1.json`: First annotator's labels
  - `annotations_annotator_2.json`: Second annotator's labels
  - `annotation_subset_real.json`: Subset for real datasets
  - `inter_rater_agreement_report.json`: Cohen's Kappa and agreement metrics
- **Purpose**: Validation for automated detection

## ğŸ”„ Data Generation

### Synthetic Data

The synthetic datasets were generated to cover various bias scenarios:

- **Demographic Parity Violations**: 40% of datasets
- **Equalized Odds Violations**: 30% of datasets
- **Equal Opportunity Violations**: 20% of datasets
- **No Bias (Control)**: 10% of datasets

**Parameters varied**:
- Sample size: 100 to 10,000
- Number of features: 5 to 20
- Protected attribute: race, sex, age
- Class imbalance: 0.1 to 0.5
- Bias strength: 0.1 to 0.9

See `datasets_metadata.csv` for complete parameters of each dataset.

### Regenerating Synthetic Data

```bash
# Generate all 500 datasets (may take 1-2 hours)
python scripts/generate_synthetic_data.py \
    --count 500 \
    --output data/synthetic/ \
    --metadata data/datasets_metadata.csv

# Generate subset for testing
python scripts/generate_synthetic_data.py \
    --count 50 \
    --output data/synthetic/test/
```

## ğŸ“¥ Downloading Case Study Datasets

Case study datasets are from public sources but need to be downloaded:

```bash
# Download all case studies
bash scripts/download_case_studies.sh

# Or individually:
python scripts/download_case_studies.py --dataset adult
python scripts/download_case_studies.py --dataset compas
python scripts/download_case_studies.py --dataset bank
python scripts/download_case_studies.py --dataset german_credit
```

## ğŸ“‹ Dataset Format

### Synthetic Datasets

Each CSV file contains:
- **Features**: `feature_0`, `feature_1`, ..., `feature_N`
- **Protected Attribute**: `sensitive_attr` (categorical)
- **Target**: `target` (binary: 0 or 1)

Example:
```csv
feature_0,feature_1,feature_2,sensitive_attr,target
0.5,0.3,0.7,group_a,1
0.2,0.8,0.1,group_b,0
...
```

### Metadata Schema

`datasets_metadata.csv` columns:
- `dataset_id`: Unique identifier (0-499)
- `filename`: CSV filename
- `n_samples`: Number of rows
- `n_features`: Number of features
- `protected_attr`: Name of protected attribute
- `bias_type`: Type of bias (demographic_parity, equalized_odds, etc.)
- `bias_strength`: Strength of bias (0.0 = none, 1.0 = maximum)
- `class_imbalance`: Ratio of minority class
- `has_bias`: Boolean indicating presence of bias

## ğŸ” Data Validation

Validate data integrity:

```bash
# Check all datasets exist
python scripts/validate_data.py --check-existence

# Validate formats and schemas
python scripts/validate_data.py --check-schema

# Compute summary statistics
python scripts/validate_data.py --summarize
```

## âš–ï¸ Ethical Considerations

- All case study datasets are from public repositories
- Datasets have been used extensively in fairness research
- Protected attributes are clearly labeled
- No personally identifiable information (PII)

## ğŸ“„ Licensing

- **Synthetic datasets**: Generated by us, MIT License
- **COMPAS**: ProPublica (CC0)
- **Adult**: UCI ML Repository (CC BY 4.0)
- **Bank**: UCI ML Repository (CC BY 4.0)
- **German Credit**: UCI ML Repository (CC BY 4.0)

## ğŸ“š References

- ProPublica COMPAS Analysis: https://github.com/propublica/compas-analysis
- UCI ML Repository: https://archive.ics.uci.edu/ml/

## ğŸ“§ Questions?

For questions about the datasets, see `docs/faq.md` or contact [your-email@domain.com]
