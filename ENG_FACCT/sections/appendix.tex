\appendix

\section{Code Examples}

\subsection{EEOC Compliance Verification}

\begin{lstlisting}[language=Python, caption=Automatic EEOC/ECOA verification]
from deepbridge import FairnessTestManager

# Automatic compliance verification
ftm = FairnessTestManager(dataset)
compliance = ftm.check_eeoc_compliance()

print(compliance['eeoc_80_rule'])  # True/False
print(compliance['eeoc_question_21'])  # True/False
print(compliance['violations'])  # List of violations
\end{lstlisting}

\subsection{Complete Analysis Workflow}

\begin{lstlisting}[language=Python, caption=Complete DeepBridge Fairness workflow]
from deepbridge import DBDataset, FairnessTestManager

# Stage 1: Create dataset with auto-detection
dataset = DBDataset(
    data=df,
    target_column='approved',
    model=trained_model
)
# Detected attributes: ['gender', 'race', 'age']

# Stage 2: Multi-dimensional analysis
ftm = FairnessTestManager(dataset)
results = ftm.run_all_tests()
# 15 metrics automatically computed

# Stage 3: EEOC/ECOA verification + optimization
compliance = ftm.check_eeoc_compliance()
optimal_threshold = ftm.optimize_threshold(
    fairness_metric='disparate_impact',
    min_accuracy=0.80
)
\end{lstlisting}

\subsection{80\% Rule Verification}

\begin{lstlisting}[language=Python, caption=Automatic 80\% rule verification]
def check_80_rule(y_pred, sensitive_attr):
    groups = sensitive_attr.unique()
    selection_rates = {}

    for group in groups:
        mask = (sensitive_attr == group)
        selection_rates[group] = y_pred[mask].mean()

    reference = max(selection_rates.values())
    violations = {}

    for group, rate in selection_rates.items():
        di = rate / reference
        if di < 0.80:
            violations[group] = {
                'DI': di,
                'selection_rate': rate,
                'reference_rate': reference,
                'shortfall': 0.80 - di
            }

    return {
        'compliant': len(violations) == 0,
        'violations': violations
    }
\end{lstlisting}

\subsection{Question 21 Verification}

\begin{lstlisting}[language=Python, caption=Question 21 verification]
def check_question_21(sensitive_attr, min_representation=0.02):
    total = len(sensitive_attr)
    warnings = {}

    for group in sensitive_attr.unique():
        count = (sensitive_attr == group).sum()
        representation = count / total

        if representation < min_representation:
            warnings[group] = {
                'count': count,
                'representation': representation,
                'required': min_representation,
                'warning': 'Insufficient sample size for statistical validity'
            }

    return {
        'valid': len(warnings) == 0,
        'warnings': warnings
    }
\end{lstlisting}

\subsection{Threshold Optimization}

\begin{lstlisting}[language=Python, caption=Multi-objective threshold optimization]
from deepbridge import FairnessTestManager

ftm = FairnessTestManager(dataset)

# Trade-off analysis in range 0.1-0.9
threshold_analysis = ftm.analyze_thresholds(
    thresholds=np.arange(0.1, 0.9, 0.05),
    fairness_metrics=['disparate_impact', 'equal_opportunity'],
    performance_metrics=['accuracy', 'f1_score']
)

# Pareto frontier: non-dominated thresholds
pareto_thresholds = threshold_analysis['pareto_frontier']

# Recommendation based on constraints
optimal = ftm.recommend_threshold(
    min_disparate_impact=0.80,
    min_accuracy=0.75,
    objective='maximize_f1'
)
\end{lstlisting}

\subsection{Pipeline Integration}

\begin{lstlisting}[language=Python, caption=Integration with complete pipeline]
from deepbridge import DBDataset, Experiment

dataset = DBDataset(df, target='approved', model=model)

# Multi-dimensional validation (fairness + robustness + uncertainty)
exp = Experiment(
    dataset=dataset,
    tests=['fairness', 'robustness', 'uncertainty']
)

results = exp.run_tests()

# Unified report with all dimensions
exp.save_pdf('complete_validation_report.pdf')
\end{lstlisting}

\subsection{CI/CD Integration}

\begin{lstlisting}[language=Python, caption=CI/CD example with fairness gates]
# .github/workflows/ml_pipeline.yml
- name: Train model
  run: python train.py

- name: Fairness testing
  run: |
    python -c "
    from deepbridge import DBDataset, FairnessTestManager

    # Load test set and model
    dataset = DBDataset(test_df, target='y', model=model)
    ftm = FairnessTestManager(dataset)

    # Verify EEOC compliance
    compliance = ftm.check_eeoc_compliance()

    # Fail pipeline if violates 80% rule
    if not compliance['eeoc_80_rule']:
        print('EEOC violation detected!')
        exit(1)
    "

- name: Deploy model
  if: success()
  run: python deploy.py
\end{lstlisting}

\subsection{Continuous Monitoring}

\begin{lstlisting}[language=Python, caption=Production fairness monitoring]
from deepbridge import FairnessMonitor

# Setup monitoring
monitor = FairnessMonitor(
    model=production_model,
    protected_attributes=['gender', 'race'],
    frequency='weekly',
    alert_threshold={'disparate_impact': 0.80}
)

# Run automatically (cron job)
report = monitor.check_fairness(production_data)

if report['violations']:
    send_alert(report)  # Email to ML team
    log_to_dashboard(report)  # Grafana/Datadog
\end{lstlisting}

\section{Metric Selection Guide}

Different regulatory and business contexts require different metrics. We offer domain-based guidance:

\subsection{Employment Screening (EEOC)}

\textbf{Regulation}: EEOC Uniform Guidelines~\cite{eeoc1978uniform}

\textbf{Mandatory Metrics}:
\begin{enumerate}
    \item \textbf{Disparate Impact}: Verify 80\% rule ($\text{DI} \geq 0.80$)
    \item \textbf{Question 21}: Validate minimum 2\% representation per group
\end{enumerate}

\textbf{Recommended Metrics}:
\begin{itemize}
    \item \textbf{Statistical Parity}: Detect subtle imbalances (< 10pp)
    \item \textbf{Equal Opportunity}: Ensure equal chances for qualified candidates
    \item \textbf{FNR Difference}: Avoid rejecting qualified candidates from protected groups
\end{itemize}

\textbf{Avoid}:
\begin{itemize}
    \item \textbf{Equalized Odds}: May force equal error rates even when differences are justified
    \item \textbf{Demographic Parity}: Overly restrictive (requires exact equality)
\end{itemize}

\subsection{Credit Scoring (ECOA)}

\textbf{Regulation}: Equal Credit Opportunity Act~\cite{ecoa1974equal}

\textbf{Mandatory Metrics}:
\begin{enumerate}
    \item \textbf{Disparate Impact}: 80\% rule applies to credit decisions
    \item \textbf{Adverse Action Notices}: Explain denial decisions
\end{enumerate}

\textbf{Recommended Metrics}:
\begin{itemize}
    \item \textbf{Equal Opportunity}: Ensure equal chances for good borrowers
    \item \textbf{Precision Parity}: Default rate should be similar among approved groups
    \item \textbf{FPR Difference}: Avoid disproportionately approving bad borrowers
\end{itemize}

\textbf{Special Consideration}:
\begin{itemize}
    \item \textbf{Risk-based pricing}: Different interest rates are allowed if based on real risk (not protected group)
    \item Relevant metric: \textbf{Calibration by group} (risk predictions should be accurate for all groups)
\end{itemize}

\subsection{Healthcare (HIPAA, AI Act)}

\textbf{Regulation}: HIPAA (USA), AI Act (EU -- upcoming)

\textbf{Recommended Metrics}:
\begin{itemize}
    \item \textbf{Equal Opportunity}: Sick patients should have equal chance of correct diagnosis
    \item \textbf{FNR Difference}: Critical -- avoid missed diagnoses in vulnerable groups
    \item \textbf{Calibration}: Risk predictions should be accurate by group
\end{itemize}

\textbf{Caution}:
\begin{itemize}
    \item \textbf{Disparate Impact can be misleading}: Higher risk prediction for vulnerable groups may reflect real health disparities (not model bias)
    \item \textbf{Domain expertise essential}: Always involve physicians in metric interpretation
\end{itemize}

\subsection{Criminal Justice}

\textbf{Regulation}: Variable by state (USA), GDPR (EU)

\textbf{Recommended Metrics}:
\begin{itemize}
    \item \textbf{Equalized Odds}: Ensure equal error rates (FPR and FNR) between groups
    \item \textbf{FPR Difference}: Critical -- avoid disproportionate false positives (as in COMPAS case)
    \item \textbf{FNR Difference}: Avoid disproportionately releasing high-risk individuals
\end{itemize}

\textbf{Inevitable Trade-off}:
\begin{itemize}
    \item If base recidivism rates differ between groups (historical reality), it is \textbf{mathematically impossible} to satisfy equalized odds AND demographic parity simultaneously~\cite{chouldechova2017fair}
    \item Political/ethical decision: Which metric to prioritize?
\end{itemize}

\section{Production Best Practices}

\subsection{CI/CD Integration}

DeepBridge Fairness can be integrated into ML pipelines (see code example in Appendix A.7).

\textbf{Fairness Gates}:
\begin{itemize}
    \item \textbf{EEOC 80\% rule}: Deployment blocked if DI < 0.80
    \item \textbf{Equalized Odds}: Warning if EOdds > 0.10
    \item \textbf{Representation}: Warning if group < 2\%
\end{itemize}

\subsection{Continuous Monitoring}

Fairness can degrade in production due to drift (see code example in Appendix A.8).

\textbf{Recommended Frequency}:
\begin{itemize}
    \item \textbf{High-risk domains} (credit, justice): Weekly
    \item \textbf{Medium-risk} (hiring): Monthly
    \item \textbf{Low-risk}: Quarterly
\end{itemize}

\subsection{Documentation and Auditing}

DeepBridge generates audit-ready reports, but \textbf{additional documentation} is recommended:

\textbf{Model Card}~\cite{mitchell2019model}:
\begin{itemize}
    \item \textbf{Intended Use}: What the model should/should not be used for
    \item \textbf{Fairness Metrics}: Report ALL 15 metrics (no cherry-picking)
    \item \textbf{Limitations}: Under-represented groups, unsatisfied metrics
    \item \textbf{Ethical Considerations}: Trade-offs, threshold decisions
\end{itemize}

\textbf{Versioning}:
\begin{itemize}
    \item Version fairness reports together with models
    \item Track how metrics change between versions
    \item Document threshold decisions and justifications
\end{itemize}

\subsection{Stakeholder Engagement}

Fairness is a sociotechnical decision, not just technical:

\textbf{Recommendations}:
\begin{enumerate}
    \item \textbf{Compliance officers}: Review EEOC/ECOA reports before deployment
    \item \textbf{Legal team}: Validate interpretation of regulations
    \item \textbf{Impacted communities}: When possible, involve representatives in metric definition
    \item \textbf{Ethics board}: Evaluate trade-offs in ambiguous cases
\end{enumerate}

\textbf{DeepBridge Visualizations for Stakeholders}:
\begin{itemize}
    \item \textbf{Pareto frontier}: Shows fairness-accuracy trade-offs visually
    \item \textbf{Radar chart}: Compares 11 metrics in accessible format
    \item \textbf{Compliance summary}: Dashboard showing EEOC/ECOA status
\end{itemize}

\section{When Not to Use DeepBridge Fairness}

DeepBridge is powerful, but not appropriate for all cases:

\textbf{Do not use when}:
\begin{enumerate}
    \item \textbf{Causal fairness is critical}: Use causal inference tools (DoWhy, CausalML)
    \item \textbf{Individual fairness required}: DeepBridge focuses on group fairness
    \item \textbf{Extremely sensitive data}: If cannot export data, use on-premise/air-gapped tools
    \item \textbf{Model is not ML}: Heuristic rules do not benefit from statistical metrics
\end{enumerate}

\textbf{Use with caution when}:
\begin{enumerate}
    \item \textbf{Very small groups} (n < 30): Confidence intervals will be wide
    \item \textbf{High intersectionality}: Manual subgroup analysis may be necessary
    \item \textbf{Biased labels}: Investigate upstream bias before trusting metrics
\end{enumerate}
