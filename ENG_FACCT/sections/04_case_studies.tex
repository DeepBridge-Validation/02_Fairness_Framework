\section{Case Studies}
\label{sec:case_studies}

We demonstrate DeepBridge Fairness effectiveness through four case studies representing regulated domains: criminal justice (COMPAS), credit (German Credit), hiring (Adult Income), and healthcare (Healthcare). For each case, we report: (1) detected violations, (2) EEOC/ECOA compliance, (3) optimal threshold, and (4) analysis time.

\subsection{Case Study 1: COMPAS -- Recidivism Prediction}

\subsubsection{Context}

COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a recidivism risk prediction system widely used in the U.S. judicial system. ProPublica investigated the system and found racial bias~\cite{angwin2016machine}.

\textbf{Dataset}: 7,214 defendants from Broward County, Florida (2013-2014)
\begin{itemize}
    \item \textbf{Target}: recidivated within 2 years (binary)
    \item \textbf{Features}: 12 (age, gender, race, criminal history)
    \item \textbf{Sensitive Attributes}: race (African-American, Caucasian, Hispanic, Other), gender (Male, Female)
    \item \textbf{Model}: Random Forest Classifier (baseline to replicate original bias)
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Auto-Detection}:
\begin{lstlisting}
dataset = DBDataset(df_compas, target='two_year_recid', model=rf_model)
print(dataset.detected_sensitive_attributes)
# ['race', 'sex', 'age']  # 100% accuracy
\end{lstlisting}

\textbf{Pre-Training Metrics}:
\begin{itemize}
    \item \textbf{Class Balance (race)}: 0.67 [WARNING] -- African-Americans have 1.5x base recidivism rate (historical confounding)
    \item \textbf{KL Divergence}: 0.23 -- Feature distributions differ significantly between races
\end{itemize}

\textbf{Post-Training Metrics} (default threshold 0.5):

\begin{table}[h]
\centering
\caption{COMPAS fairness metrics by race (threshold 0.5)}
\label{tab:compas_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{African-American} & \textbf{Caucasian} & \textbf{Difference} \\
\midrule
Statistical Parity & 0.59 & 0.38 & 0.21 [VIOLATION] \\
Disparate Impact & 1.55 & 1.00 & -- \\
Equal Opportunity & 0.72 & 0.65 & 0.07 \\
FNR Difference & 0.28 & 0.35 & -0.07 \\
FPR Difference & 0.45 & 0.23 & 0.22 [VIOLATION] \\
Precision & 0.63 & 0.71 & -0.08 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Detected Violations}:
\begin{enumerate}
    \item \textbf{Statistical Parity}: 21pp difference (threshold: <10pp)
    \item \textbf{Disparate Impact}: DI=1.55 (does not violate 80\% rule, but favors African-Americans in selection)
    \item \textbf{FPR Difference}: 22pp -- African-Americans have 2x False Positive rate (the critical bias identified by ProPublica)
\end{enumerate}

\textbf{EEOC Verification}:
\begin{verbatim}
EEOC 80% Rule: NOT APPLICABLE (system is not "selection")
Note: COMPAS is not a hiring system, but a risk
assessment system. 80% rule does not formally apply.

Fairness Concern: Equalized Odds violated (FPR disparity)
Recommendation: Equalize FPR via threshold adjustment
\end{verbatim}

\textbf{Threshold Optimization}:

DeepBridge identified optimal threshold = \textbf{0.62} that:
\begin{itemize}
    \item Reduces FPR difference from 22pp â†’ 8pp
    \item Maintains accuracy above 68\%
    \item Equalized Odds: EOdds = 0.09 (< threshold 0.10)
\end{itemize}

\textbf{Analysis Time}: \textbf{7.2 minutes} (vs. 35 minutes with AI Fairness 360 + manual analysis)

\subsection{Case Study 2: German Credit -- Credit Scoring}

\subsubsection{Context}

German Credit dataset is a classic benchmark for credit scoring~\cite{dua2017uci}. Applicable to ECOA (Equal Credit Opportunity Act).

\textbf{Dataset}: 1,000 customers from a German bank
\begin{itemize}
    \item \textbf{Target}: good credit (binary)
    \item \textbf{Features}: 20 (age, marital status, credit history, employment)
    \item \textbf{Sensitive Attributes}: age (< 25, 25-60, >60), sex (male, female), foreign\_worker (yes, no)
    \item \textbf{Model}: XGBoost Classifier
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Auto-Detection}:
\begin{lstlisting}
dataset = DBDataset(df_credit, target='credit_risk', model=xgb_model)
print(dataset.detected_sensitive_attributes)
# ['age', 'sex', 'foreign_worker']  # 100% accuracy
\end{lstlisting}

\textbf{Post-Training Metrics} (by age):

\begin{table}[h]
\centering
\caption{German Credit fairness metrics by age (threshold 0.5)}
\label{tab:credit_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{<25} & \textbf{25-60} & \textbf{>60} \\
\midrule
Approval Rate & 0.52 & 0.71 & 0.68 \\
Disparate Impact & 0.73 [VIOLATION] & 1.00 & 0.96 \\
Equal Opportunity & 0.58 & 0.72 & 0.70 \\
Precision & 0.65 & 0.78 & 0.75 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{ECOA Verification}:
\begin{verbatim}
ECOA Compliance Check:
- Age <25: DI = 0.73 [VIOLATION OF 80% RULE]
- Selection rate: 52% vs. 71% (reference)
- Shortfall: 7pp to reach 80% threshold

Action Required:
- Adjust threshold OR retrain model with fairness constraints
- Generate adverse action notices for denied applicants

Sample Adverse Action Notice:
"Your credit application was denied. Primary reasons:
  1. Insufficient credit history (score: 320/800)
  2. High debt-to-income ratio (45% vs. recommended <36%)"
\end{verbatim}

\textbf{Threshold Optimization}:

Pareto frontier identified 3 candidate thresholds:
\begin{enumerate}
    \item \textbf{t=0.38}: DI=0.82 [COMPLIANT], Accuracy=69\%
    \item \textbf{t=0.45}: DI=0.80 [BARELY COMPLIANT], Accuracy=72\%
    \item \textbf{t=0.50}: DI=0.73 [VIOLATION], Accuracy=74\%
\end{enumerate}

\textbf{Recommendation}: t=0.45 balances ECOA compliance with acceptable performance.

\textbf{Analysis Time}: \textbf{5.8 minutes}

\subsection{Adult Income and Healthcare (Summary)}

\textbf{Adult Income.} Census data (N=48,842) for employment screening showed severe EEOC violation with demographic impact DI=0.43 for females vs. males (threshold: 0.80). Auto-detection identified 5/5 sensitive attributes (sex, race, age, marital status, native country). Threshold optimization explored range 10-90\% but could not achieve simultaneous fairness and accuracy (DI max=0.65 at 25\% threshold with 12\% accuracy loss), indicating model retraining needed.

\textbf{Healthcare.} Hospital readmission prediction (N=10,000) revealed higher risk scores for minority patients (TPR disparity 0.15). EEOC verification flagged representation concerns (African American=8\%, threshold=2\%). Unlike other cases, threshold adjustment was inappropriate due to clinical implications, demonstrating DeepBridge's value in surfacing fairness-accuracy trade-offs requiring domain expertise.

Table~\ref{tab:case_studies_comparison} summarizes all four case studies.

\subsection{Case Studies Synthesis}

\begin{table}[h]
\centering
\caption{Comparative summary of case studies}
\label{tab:case_studies_comparison}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{COMPAS} & \textbf{Credit} & \textbf{Adult} & \textbf{Health} \\
\midrule
Detected attributes & 3/3 & 3/3 & 2/2 & 2/2 \\
EEOC/ECOA violations & 1 & 1 & 2 & N/A \\
Adjustable threshold? & Yes & Yes & Limited & No \\
Analysis time (min) & 7.2 & 5.8 & 12.4 & 9.1 \\
Manual time (min) & 35 & 25 & 50 & 40 \\
Time savings & 79\% & 77\% & 75\% & 77\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Insights}:
\begin{enumerate}
    \item \textbf{100\% accurate auto-detection}: All sensitive attributes detected in all datasets
    \item \textbf{Frequent violations}: 3/4 cases violate 80\% rule or equalized odds
    \item \textbf{Context matters}: Healthcare requires clinical analysis, not just threshold adjustment
    \item \textbf{Consistent savings}: 75-79\% time reduction vs. manual analysis
\end{enumerate}
