{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Fairness Framework - Quick Demo\n",
    "\n",
    "**Duration**: 5-10 minutes\n",
    "\n",
    "This notebook demonstrates the basic usage of the Fairness Framework for detecting bias in machine learning models.\n",
    "\n",
    "## What you'll learn:\n",
    "1. How to load data\n",
    "2. How to detect bias automatically\n",
    "3. How to interpret fairness metrics\n",
    "4. How to visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fairness framework\n",
    "from src.fairness_detector import FairnessDetector\n",
    "from src.metrics import compute_all_metrics\n",
    "from src.visualization import plot_fairness_report, plot_group_comparison\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Data (for demonstration)\n",
    "\n",
    "Let's create a simple dataset with intentional bias for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset with bias\n",
    "n_samples = 1000\n",
    "\n",
    "# Features\n",
    "feature_1 = np.random.randn(n_samples)\n",
    "feature_2 = np.random.randn(n_samples)\n",
    "\n",
    "# Sensitive attribute (binary: Group A vs Group B)\n",
    "sensitive_attr = np.random.choice(['Group_A', 'Group_B'], size=n_samples)\n",
    "\n",
    "# Create target with intentional bias\n",
    "# Group A has higher probability of positive outcome\n",
    "target = []\n",
    "for i in range(n_samples):\n",
    "    if sensitive_attr[i] == 'Group_A':\n",
    "        prob = 0.7  # 70% positive rate for Group A\n",
    "    else:\n",
    "        prob = 0.4  # 40% positive rate for Group B (BIAS!)\n",
    "    \n",
    "    # Add some randomness based on features\n",
    "    prob += 0.1 * feature_1[i] + 0.05 * feature_2[i]\n",
    "    prob = np.clip(prob, 0, 1)\n",
    "    \n",
    "    target.append(1 if np.random.rand() < prob else 0)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': feature_1,\n",
    "    'feature_2': feature_2,\n",
    "    'sensitive_attr': sensitive_attr,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "print(f\"Dataset created: {df.shape[0]} samples, {df.shape[1]} columns\")\n",
    "print(f\"\\nTarget distribution by group:\")\n",
    "print(df.groupby('sensitive_attr')['target'].agg(['count', 'mean', 'sum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Fairness Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detector instance\n",
    "detector = FairnessDetector(threshold=0.1, verbose=True)\n",
    "\n",
    "# Set configuration\n",
    "detector.set_sensitive_attributes(['sensitive_attr'])\n",
    "detector.set_target('target')\n",
    "\n",
    "print(\"\\nâœ“ Detector configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detect Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bias detection\n",
    "results = detector.detect_bias(df)\n",
    "\n",
    "# Print summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute All Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all available metrics\n",
    "all_metrics = compute_all_metrics(\n",
    "    y_true=df['target'],\n",
    "    y_pred=df['target'],  # Using actual labels for simplicity\n",
    "    sensitive_attr=df['sensitive_attr']\n",
    ")\n",
    "\n",
    "print(\"All Fairness Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric_name, value in all_metrics.items():\n",
    "    status = \"âœ“ FAIR\" if abs(value) <= 0.1 else \"âœ— BIASED\"\n",
    "    print(f\"{metric_name:25s}: {value:7.4f}  [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive fairness report\n",
    "plot_fairness_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot group comparison\n",
    "plot_group_comparison(\n",
    "    y_pred=df['target'],\n",
    "    sensitive_attr=df['sensitive_attr'],\n",
    "    group_names=['Group A', 'Group B']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretation\n",
    "\n",
    "### What do these metrics mean?\n",
    "\n",
    "- **Demographic Parity**: Difference in positive prediction rates between groups\n",
    "  - Value close to 0 = Fair\n",
    "  - |value| > 0.1 = Potential bias\n",
    "\n",
    "- **Equalized Odds**: Maximum difference in TPR or FPR between groups\n",
    "  - Ensures similar error rates across groups\n",
    "\n",
    "- **Equal Opportunity**: Difference in true positive rates\n",
    "  - Ensures equal benefit to qualified individuals\n",
    "\n",
    "- **Disparate Impact**: Ratio of positive rates\n",
    "  - Ratio < 0.8 often indicates bias (80% rule)\n",
    "\n",
    "### Our Results:\n",
    "\n",
    "In this demo, we created data with **intentional bias** where Group A has a 70% positive rate while Group B has only 40%. The framework successfully detected this bias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've seen the basics, try:\n",
    "\n",
    "1. **Load real data**: Use one of our case studies\n",
    "   ```python\n",
    "   df = pd.read_csv('../../data/case_studies/adult/adult.csv')\n",
    "   ```\n",
    "\n",
    "2. **Try different metrics**: Explore all available fairness metrics\n",
    "\n",
    "3. **Experiment with thresholds**: Change the bias detection threshold\n",
    "   ```python\n",
    "   detector.set_threshold(0.05)  # Stricter threshold\n",
    "   ```\n",
    "\n",
    "4. **Multiple protected attributes**: Analyze intersectional fairness\n",
    "   ```python\n",
    "   detector.set_sensitive_attributes(['race', 'sex'])\n",
    "   ```\n",
    "\n",
    "5. **Check out other notebooks**:\n",
    "   - `02_experiment_1.ipynb`: Full auto-detection experiment\n",
    "   - `03_visualization.ipynb`: Advanced visualizations\n",
    "   - `04_case_studies.ipynb`: Real-world examples\n",
    "\n",
    "## ðŸ“š Resources\n",
    "\n",
    "- **Documentation**: See `../../docs/`\n",
    "- **Paper**: `../../paper/main/main.pdf`\n",
    "- **API Reference**: `../../docs/api/`\n",
    "- **Examples**: `../../experiments/scripts/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Congratulations!\n",
    "\n",
    "You've completed the quick demo. You now know how to:\n",
    "- âœ“ Initialize the FairnessDetector\n",
    "- âœ“ Detect bias in datasets\n",
    "- âœ“ Interpret fairness metrics\n",
    "- âœ“ Visualize results\n",
    "\n",
    "**Ready to detect bias in your ML models?** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
