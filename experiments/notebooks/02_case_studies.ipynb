{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Fairness Framework - Real-World Case Studies\n",
    "\n",
    "**Duration**: 30-45 minutes\n",
    "\n",
    "This notebook demonstrates the application of the Fairness Framework to real-world datasets:\n",
    "1. **COMPAS** - Recidivism prediction\n",
    "2. **Adult Income** - Income classification\n",
    "3. **German Credit** - Credit risk assessment\n",
    "4. **Bank Marketing** - Marketing campaign outcomes\n",
    "\n",
    "## What you'll learn:\n",
    "- How to analyze real-world datasets for bias\n",
    "- How to check regulatory compliance (EEOC/ECOA)\n",
    "- How to interpret results in practical contexts\n",
    "- How to compare fairness across different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fairness framework\n",
    "from src.fairness_detector import FairnessDetector\n",
    "from src.metrics import compute_all_metrics, is_fair\n",
    "from src.visualization import plot_fairness_report, plot_group_comparison, create_fairness_dashboard\n",
    "from src.utils import group_statistics\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1: COMPAS - Recidivism Prediction\n",
    "\n",
    "**Context**: COMPAS is a tool used in the US criminal justice system to predict recidivism risk.\n",
    "\n",
    "**Known Issues**: ProPublica investigation found racial bias in predictions.\n",
    "\n",
    "**Protected Attributes**: Race, Sex, Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a placeholder. Actual COMPAS data should be loaded from data/case_studies/compas/\n",
    "# For demonstration, we'll create synthetic data with known bias patterns\n",
    "\n",
    "print(\"ðŸ“ Loading COMPAS dataset...\")\n",
    "print(\"âš ï¸  Note: Using synthetic data for demonstration\")\n",
    "print(\"   Real COMPAS data should be placed in: data/case_studies/compas/\")\n",
    "\n",
    "# Create synthetic COMPAS-like data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "compas_data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'race': np.random.choice(['African-American', 'Caucasian', 'Hispanic'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "    'sex': np.random.choice(['Male', 'Female'], n_samples, p=[0.7, 0.3]),\n",
    "    'priors_count': np.random.poisson(2, n_samples),\n",
    "    'risk_score': np.random.randint(1, 11, n_samples),\n",
    "})\n",
    "\n",
    "# Create biased predictions (higher risk scores for African-American)\n",
    "compas_data['high_risk'] = (compas_data['risk_score'] >= 5).astype(int)\n",
    "# Add bias\n",
    "mask = compas_data['race'] == 'African-American'\n",
    "compas_data.loc[mask & (np.random.rand(n_samples) < 0.3), 'high_risk'] = 1\n",
    "\n",
    "print(f\"âœ“ Loaded {len(compas_data)} records\")\n",
    "print(f\"\\nRace distribution:\")\n",
    "print(compas_data['race'].value_counts())\n",
    "print(f\"\\nHigh risk rate by race:\")\n",
    "print(compas_data.groupby('race')['high_risk'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze COMPAS for bias\n",
    "print(\"ðŸ” Analyzing COMPAS dataset for racial bias...\\n\")\n",
    "\n",
    "detector = FairnessDetector(threshold=0.1, verbose=False)\n",
    "detector.load_data(\n",
    "    compas_data,\n",
    "    target='high_risk',\n",
    "    sensitive_attrs=['race']\n",
    ")\n",
    "\n",
    "results = detector.detect_bias()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check EEOC compliance for COMPAS\n",
    "print(\"âš–ï¸  Checking EEOC Compliance...\\n\")\n",
    "\n",
    "compliance = detector.check_eeoc_compliance()\n",
    "\n",
    "print(f\"Compliant: {compliance['compliant']}\")\n",
    "print(f\"Impact Ratio: {compliance['impact_ratio']:.3f} (threshold: 0.80)\")\n",
    "print(f\"\\nSelection Rates by Race:\")\n",
    "for group, rate in compliance['selection_rates'].items():\n",
    "    print(f\"  {group}: {rate:.3f}\")\n",
    "\n",
    "if not compliance['compliant']:\n",
    "    print(f\"\\nâš ï¸  Failing Groups: {', '.join(compliance['failing_groups'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize COMPAS results\n",
    "plot_group_comparison(\n",
    "    y_pred=compas_data['high_risk'],\n",
    "    sensitive_attr=compas_data['race'],\n",
    "    group_names=compas_data['race'].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: Adult Income Dataset\n",
    "\n",
    "**Context**: Predict whether income exceeds $50K/year based on census data.\n",
    "\n",
    "**Protected Attributes**: Sex, Race\n",
    "\n",
    "**Use Case**: Employment screening, lending decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Loading Adult Income dataset...\")\n",
    "print(\"âš ï¸  Note: Using synthetic data for demonstration\")\n",
    "print(\"   Real Adult dataset should be placed in: data/case_studies/adult/\")\n",
    "\n",
    "# Create synthetic Adult-like data\n",
    "n_samples = 1500\n",
    "adult_data = pd.DataFrame({\n",
    "    'age': np.random.randint(17, 90, n_samples),\n",
    "    'education': np.random.randint(1, 17, n_samples),\n",
    "    'hours_per_week': np.random.randint(1, 99, n_samples),\n",
    "    'sex': np.random.choice(['Male', 'Female'], n_samples, p=[0.67, 0.33]),\n",
    "    'race': np.random.choice(['White', 'Black', 'Asian', 'Other'], n_samples, p=[0.75, 0.12, 0.08, 0.05]),\n",
    "})\n",
    "\n",
    "# Create income prediction with gender bias\n",
    "adult_data['high_income'] = (\n",
    "    (adult_data['education'] > 10) & \n",
    "    (adult_data['hours_per_week'] > 35)\n",
    ").astype(int)\n",
    "\n",
    "# Add gender bias\n",
    "mask = adult_data['sex'] == 'Female'\n",
    "adult_data.loc[mask & (np.random.rand(n_samples) < 0.25), 'high_income'] = 0\n",
    "\n",
    "print(f\"âœ“ Loaded {len(adult_data)} records\")\n",
    "print(f\"\\nHigh income rate by sex:\")\n",
    "print(adult_data.groupby('sex')['high_income'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Adult dataset\n",
    "print(\"ðŸ” Analyzing Adult Income dataset for gender bias...\\n\")\n",
    "\n",
    "detector_adult = FairnessDetector(threshold=0.1)\n",
    "detector_adult.load_data(\n",
    "    adult_data,\n",
    "    target='high_income',\n",
    "    sensitive_attrs=['sex']\n",
    ")\n",
    "\n",
    "results_adult = detector_adult.detect_bias()\n",
    "print(results_adult.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 3: German Credit Risk\n",
    "\n",
    "**Context**: Assess credit risk for loan applicants.\n",
    "\n",
    "**Protected Attributes**: Sex, Age\n",
    "\n",
    "**Regulation**: ECOA compliance required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Loading German Credit dataset...\")\n",
    "print(\"âš ï¸  Note: Using synthetic data for demonstration\")\n",
    "\n",
    "# Create synthetic German Credit-like data\n",
    "n_samples = 800\n",
    "credit_data = pd.DataFrame({\n",
    "    'age': np.random.randint(19, 75, n_samples),\n",
    "    'sex': np.random.choice(['male', 'female'], n_samples, p=[0.7, 0.3]),\n",
    "    'credit_amount': np.random.randint(250, 18500, n_samples),\n",
    "    'duration': np.random.randint(4, 72, n_samples),\n",
    "    'credit_history': np.random.choice(['good', 'poor', 'critical'], n_samples, p=[0.6, 0.3, 0.1]),\n",
    "})\n",
    "\n",
    "# Create credit approval with age bias\n",
    "credit_data['approved'] = (\n",
    "    (credit_data['credit_history'] == 'good') & \n",
    "    (credit_data['credit_amount'] < 10000)\n",
    ").astype(int)\n",
    "\n",
    "# Add age bias (discriminate against younger applicants)\n",
    "mask = credit_data['age'] < 25\n",
    "credit_data.loc[mask & (np.random.rand(n_samples) < 0.3), 'approved'] = 0\n",
    "\n",
    "# Create age groups for analysis\n",
    "credit_data['age_group'] = pd.cut(credit_data['age'], bins=[0, 25, 60, 100], labels=['young', 'middle', 'senior'])\n",
    "\n",
    "print(f\"âœ“ Loaded {len(credit_data)} records\")\n",
    "print(f\"\\nApproval rate by age group:\")\n",
    "print(credit_data.groupby('age_group')['approved'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze German Credit for ECOA compliance\n",
    "print(\"ðŸ” Analyzing German Credit dataset...\\n\")\n",
    "\n",
    "detector_credit = FairnessDetector(threshold=0.1)\n",
    "detector_credit.load_data(\n",
    "    credit_data,\n",
    "    target='approved',\n",
    "    sensitive_attrs=['age_group']\n",
    ")\n",
    "\n",
    "# Check ECOA compliance\n",
    "ecoa_compliance = detector_credit.check_ecoa_compliance()\n",
    "\n",
    "print(\"âš–ï¸  ECOA Compliance Check:\")\n",
    "print(f\"\\nCompliant: {ecoa_compliance['compliant']}\")\n",
    "print(f\"Disparate Impact: {ecoa_compliance['disparate_impact']:.3f}\")\n",
    "print(f\"\\nApproval Rates:\")\n",
    "for group, rate in ecoa_compliance['approval_rates'].items():\n",
    "    print(f\"  {group}: {rate:.3f}\")\n",
    "\n",
    "if ecoa_compliance['disadvantaged_groups']:\n",
    "    print(f\"\\nâš ï¸  Disadvantaged Groups: {', '.join(ecoa_compliance['disadvantaged_groups'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Let's compare fairness across all case studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all datasets\n",
    "print(\"ðŸ“Š Computing fairness metrics for all case studies...\\n\")\n",
    "\n",
    "# COMPAS metrics\n",
    "compas_metrics = compute_all_metrics(\n",
    "    compas_data['high_risk'],\n",
    "    compas_data['high_risk'],\n",
    "    compas_data['race']\n",
    ")\n",
    "\n",
    "# Adult metrics\n",
    "adult_metrics = compute_all_metrics(\n",
    "    adult_data['high_income'],\n",
    "    adult_data['high_income'],\n",
    "    adult_data['sex']\n",
    ")\n",
    "\n",
    "# Credit metrics\n",
    "credit_metrics = compute_all_metrics(\n",
    "    credit_data['approved'],\n",
    "    credit_data['approved'],\n",
    "    credit_data['age_group']\n",
    ")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'COMPAS (Race)': compas_metrics,\n",
    "    'Adult (Sex)': adult_metrics,\n",
    "    'Credit (Age)': credit_metrics\n",
    "})\n",
    "\n",
    "print(\"Fairness Metrics Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.round(3))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check which ones are fair\n",
    "print(\"\\nFairness Assessment (threshold: 0.1):\")\n",
    "for metric in comparison.index:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for dataset in comparison.columns:\n",
    "        value = comparison.loc[metric, dataset]\n",
    "        fair = is_fair(value, metric)\n",
    "        status = \"âœ“ FAIR\" if fair else \"âœ— BIASED\"\n",
    "        print(f\"  {dataset:20s}: {value:7.3f}  [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Different Contexts, Different Biases\n",
    "- **COMPAS**: Racial bias in criminal justice\n",
    "- **Adult Income**: Gender bias in income prediction\n",
    "- **German Credit**: Age discrimination in lending\n",
    "\n",
    "### 2. Regulatory Considerations\n",
    "- **EEOC**: Applies to employment decisions (Adult Income)\n",
    "- **ECOA**: Applies to credit decisions (German Credit)\n",
    "- **Criminal Justice**: Different standards (COMPAS)\n",
    "\n",
    "### 3. Mitigation Strategies\n",
    "1. **Data Collection**: Ensure representative sampling\n",
    "2. **Feature Engineering**: Remove or transform biased features\n",
    "3. **Algorithmic Adjustments**: Apply fairness constraints\n",
    "4. **Post-processing**: Adjust predictions to achieve fairness\n",
    "5. **Ongoing Monitoring**: Regular audits and updates\n",
    "\n",
    "### 4. Trade-offs\n",
    "- Fairness vs. Accuracy\n",
    "- Different fairness metrics may conflict\n",
    "- Context-specific considerations\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Load Real Data**: Replace synthetic data with actual case study datasets\n",
    "2. **Try Mitigation**: Experiment with bias mitigation techniques\n",
    "3. **Compare Tools**: Benchmark against AIF360, Fairlearn\n",
    "4. **Domain Expertise**: Consult with domain experts for interpretation\n",
    "5. **Continuous Monitoring**: Implement ongoing fairness audits\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **COMPAS Analysis**: ProPublica (2016)\n",
    "- **Adult Dataset**: UCI ML Repository\n",
    "- **German Credit**: UCI ML Repository\n",
    "- **Regulations**: EEOC, ECOA, GDPR, EU AI Act\n",
    "- **Tools**: AIF360, Fairlearn, Aequitas\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Fairness is context-dependent and requires domain expertise. Always consult with stakeholders and legal experts when making decisions based on fairness analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
