\section{Case Studies}
\label{sec:case_studies}

We demonstrate DeepBridge Fairness effectiveness through four case studies representing regulated domains: criminal justice (COMPAS), credit (German Credit), hiring (Adult Income), and healthcare (Healthcare). For each case, we report: (1) detected violations, (2) EEOC/ECOA compliance, (3) optimal threshold, and (4) analysis time.

\subsection{Case Study 1: COMPAS -- Recidivism Prediction}

\subsubsection{Context}

COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a recidivism risk prediction system widely used in the U.S. judicial system. ProPublica investigated the system and found racial bias~\cite{angwin2016machine}.

\textbf{Dataset}: 7,214 defendants from Broward County, Florida (2013-2014)
\begin{itemize}
    \item \textbf{Target}: recidivated within 2 years (binary)
    \item \textbf{Features}: 12 (age, gender, race, criminal history)
    \item \textbf{Sensitive Attributes}: race (African-American, Caucasian, Hispanic, Other), gender (Male, Female)
    \item \textbf{Model}: Random Forest Classifier (baseline to replicate original bias)
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Auto-Detection}:
\begin{lstlisting}
dataset = DBDataset(df_compas, target='two_year_recid', model=rf_model)
print(dataset.detected_sensitive_attributes)
# ['race', 'sex', 'age']  # 100% accuracy
\end{lstlisting}

\textbf{Pre-Training Metrics}:
\begin{itemize}
    \item \textbf{Class Balance (race)}: 0.67 [WARNING] -- African-Americans have 1.5x base recidivism rate (historical confounding)
    \item \textbf{KL Divergence}: 0.23 -- Feature distributions differ significantly between races
\end{itemize}

\textbf{Post-Training Metrics} (default threshold 0.5):

\begin{table}[h]
\centering
\caption{COMPAS fairness metrics by race (threshold 0.5)}
\label{tab:compas_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{African-American} & \textbf{Caucasian} & \textbf{Difference} \\
\midrule
Statistical Parity & 0.59 & 0.38 & 0.21 [VIOLATION] \\
Disparate Impact & 1.55 & 1.00 & -- \\
Equal Opportunity & 0.72 & 0.65 & 0.07 \\
FNR Difference & 0.28 & 0.35 & -0.07 \\
FPR Difference & 0.45 & 0.23 & 0.22 [VIOLATION] \\
Precision & 0.63 & 0.71 & -0.08 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Detected Violations}:
\begin{enumerate}
    \item \textbf{Statistical Parity}: 21pp difference (threshold: <10pp)
    \item \textbf{Disparate Impact}: DI=1.55 (does not violate 80\% rule, but favors African-Americans in selection)
    \item \textbf{FPR Difference}: 22pp -- African-Americans have 2x False Positive rate (the critical bias identified by ProPublica)
\end{enumerate}

\textbf{EEOC Verification}:
\begin{verbatim}
EEOC 80% Rule: NOT APPLICABLE (system is not "selection")
Note: COMPAS is not a hiring system, but a risk
assessment system. 80% rule does not formally apply.

Fairness Concern: Equalized Odds violated (FPR disparity)
Recommendation: Equalize FPR via threshold adjustment
\end{verbatim}

\textbf{Threshold Optimization}:

DeepBridge identified optimal threshold = \textbf{0.62} that:
\begin{itemize}
    \item Reduces FPR difference from 22pp â†’ 8pp
    \item Maintains accuracy above 68\%
    \item Equalized Odds: EOdds = 0.09 (< threshold 0.10)
\end{itemize}

\textbf{Analysis Time}: \textbf{7.2 minutes} (vs. 35 minutes with AI Fairness 360 + manual analysis)

\subsection{Case Study 2: German Credit -- Credit Scoring}

\subsubsection{Context}

German Credit dataset is a classic benchmark for credit scoring~\cite{dua2017uci}. Applicable to ECOA (Equal Credit Opportunity Act).

\textbf{Dataset}: 1,000 customers from a German bank
\begin{itemize}
    \item \textbf{Target}: good credit (binary)
    \item \textbf{Features}: 20 (age, marital status, credit history, employment)
    \item \textbf{Sensitive Attributes}: age (< 25, 25-60, >60), sex (male, female), foreign\_worker (yes, no)
    \item \textbf{Model}: XGBoost Classifier
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Auto-Detection}:
\begin{lstlisting}
dataset = DBDataset(df_credit, target='credit_risk', model=xgb_model)
print(dataset.detected_sensitive_attributes)
# ['age', 'sex', 'foreign_worker']  # 100% accuracy
\end{lstlisting}

\textbf{Post-Training Metrics} (by age):

\begin{table}[h]
\centering
\caption{German Credit fairness metrics by age (threshold 0.5)}
\label{tab:credit_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{<25} & \textbf{25-60} & \textbf{>60} \\
\midrule
Approval Rate & 0.52 & 0.71 & 0.68 \\
Disparate Impact & 0.73 [VIOLATION] & 1.00 & 0.96 \\
Equal Opportunity & 0.58 & 0.72 & 0.70 \\
Precision & 0.65 & 0.78 & 0.75 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{ECOA Verification}:
\begin{verbatim}
ECOA Compliance Check:
- Age <25: DI = 0.73 [VIOLATION OF 80% RULE]
- Selection rate: 52% vs. 71% (reference)
- Shortfall: 7pp to reach 80% threshold

Action Required:
- Adjust threshold OR retrain model with fairness constraints
- Generate adverse action notices for denied applicants

Sample Adverse Action Notice:
"Your credit application was denied. Primary reasons:
  1. Insufficient credit history (score: 320/800)
  2. High debt-to-income ratio (45% vs. recommended <36%)"
\end{verbatim}

\textbf{Threshold Optimization}:

Pareto frontier identified 3 candidate thresholds:
\begin{enumerate}
    \item \textbf{t=0.38}: DI=0.82 [COMPLIANT], Accuracy=69\%
    \item \textbf{t=0.45}: DI=0.80 [BARELY COMPLIANT], Accuracy=72\%
    \item \textbf{t=0.50}: DI=0.73 [VIOLATION], Accuracy=74\%
\end{enumerate}

\textbf{Recommendation}: t=0.45 balances ECOA compliance with acceptable performance.

\textbf{Analysis Time}: \textbf{5.8 minutes}

\subsection{Case Study 3: Adult Income -- Employment Screening}

\subsubsection{Context}

Adult Income dataset (UCI) predicts if individual earns >50K/year~\cite{dua2017uci}. Commonly used as proxy for hiring decisions (EEOC applicable).

\textbf{Dataset}: 48,842 individuals from US Census (1994)
\begin{itemize}
    \item \textbf{Target}: income >50K (binary)
    \item \textbf{Features}: 14 (age, education, occupation, race, sex, country of origin)
    \item \textbf{Sensitive Attributes}: sex (Male, Female), race (White, Black, Asian-Pac-Islander, Amer-Indian-Eskimo, Other)
    \item \textbf{Model}: LightGBM Classifier
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Post-Training Metrics} (by sex):

\begin{table}[h]
\centering
\caption{Adult Income fairness metrics by sex (threshold 0.5)}
\label{tab:adult_metrics}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Female} & \textbf{Male} \\
\midrule
Predicted High Income \% & 14.2\% & 32.8\% \\
Disparate Impact & 0.43 [VIOLATION] & 1.00 \\
Equal Opportunity & 0.48 & 0.71 \\
Equalized Odds & 0.23 [VIOLATION] & -- \\
Accuracy & 83.5\% & 85.2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{EEOC Verification}:
\begin{verbatim}
EEOC 80% Rule Verification:
- Female: DI = 0.43 [SEVERE VIOLATION]
- Selection rate: 14.2% vs. 32.8% (Male)
- Shortfall: 37pp to reach 80% threshold

EEOC Question 21:
- Female: 32.4% representation [VALID]
- Male: 67.6% representation [VALID]

Risk Assessment: HIGH
- Severe disparate impact
- Would likely face EEOC challenge if deployed
\end{verbatim}

\textbf{Root Cause Analysis}:

DeepBridge analyzes feature importance by group:
\begin{itemize}
    \item \textbf{Female}: Top features = [education, hours\_per\_week, occupation]
    \item \textbf{Male}: Top features = [occupation, age, capital\_gain]
    \item \textbf{Bias Source}: ``occupation'' is proxy for gender (nurses=F, engineers=M)
\end{itemize}

\textbf{Mitigation Recommendation}:
\begin{enumerate}
    \item \textbf{Threshold adjustment}: Insufficient (DI max = 0.65 even with t=0.1)
    \item \textbf{Reweighting}: Train with sample weights balancing groups
    \item \textbf{Adversarial debiasing}: Add adversary penalizing gender predictions
\end{enumerate}

\textbf{Analysis Time}: \textbf{12.4 minutes} (larger dataset)

\subsection{Case Study 4: Healthcare Risk Prediction}

\subsubsection{Context}

Hospital readmission risk prediction within 30 days. Regulated by HIPAA and soon by AI Act (EU).

\textbf{Dataset}: 10,000 hospital patients (synthetic data based on MIMIC-III)
\begin{itemize}
    \item \textbf{Target}: readmission within 30 days (binary)
    \item \textbf{Features}: 25 (age, race, diagnoses, comorbidities)
    \item \textbf{Sensitive Attributes}: race (White, Black, Hispanic, Asian), age\_group (<50, 50-70, >70)
    \item \textbf{Model}: Neural Network (3 layers, 128-64-32 neurons)
\end{itemize}

\subsubsection{DeepBridge Analysis}

\textbf{Post-Training Metrics} (by race):

\begin{table}[h]
\centering
\caption{Healthcare fairness metrics by race (threshold 0.5)}
\label{tab:health_metrics}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{White} & \textbf{Black} & \textbf{Hispanic} & \textbf{Asian} \\
\midrule
Predicted Readmission & 22\% & 31\% & 28\% & 19\% \\
Disparate Impact & 1.00 & 1.41 & 1.27 & 0.86 \\
Equal Opportunity & 0.68 & 0.75 & 0.71 & 0.65 \\
FNR (miss risk) & 0.32 & 0.25 & 0.29 & 0.35 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical Ethical Question}:

Model predicts \textit{higher} risk for Black/Hispanic patients. Possible causes:
\begin{enumerate}
    \item \textbf{Historical bias}: Real disparities in healthcare access (model reflects unjust reality)
    \item \textbf{Proxy features}: Zip code, insurance type are proxies for race
    \item \textbf{Label bias}: Readmissions may be influenced by physician bias in admissions
\end{enumerate}

\textbf{DeepBridge Recommendation}:
\begin{verbatim}
WARNING: Clinical Context Required
- Higher predicted risk for minority groups detected
- Possible causes: (1) legitimate health disparities OR
  (2) biased features/labels
- Action: Clinical review of feature importance
- Consider: Remove zip_code, insurance_type
- Monitor: Real-world outcomes by race after deployment
\end{verbatim}

\textbf{Threshold Optimization}: NOT RECOMMENDED in this case
\begin{itemize}
    \item Adjusting threshold may \textit{reduce} risk detection in vulnerable groups
    \item Potential harm: High-risk patients do not receive preventive interventions
    \item Preferred approach: Mitigation via feature engineering, not threshold
\end{itemize}

\textbf{Analysis Time}: \textbf{9.1 minutes}

\subsection{Case Studies Synthesis}

\begin{table}[h]
\centering
\caption{Comparative summary of case studies}
\label{tab:case_summary}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{COMPAS} & \textbf{Credit} & \textbf{Adult} & \textbf{Health} \\
\midrule
Detected attributes & 3/3 & 3/3 & 2/2 & 2/2 \\
EEOC/ECOA violations & 1 & 1 & 2 & N/A \\
Adjustable threshold? & Yes & Yes & Limited & No \\
Analysis time (min) & 7.2 & 5.8 & 12.4 & 9.1 \\
Manual time (min) & 35 & 25 & 50 & 40 \\
Time savings & 79\% & 77\% & 75\% & 77\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Insights}:
\begin{enumerate}
    \item \textbf{100\% accurate auto-detection}: All sensitive attributes detected in all datasets
    \item \textbf{Frequent violations}: 3/4 cases violate 80\% rule or equalized odds
    \item \textbf{Context matters}: Healthcare requires clinical analysis, not just threshold adjustment
    \item \textbf{Consistent savings}: 75-79\% time reduction vs. manual analysis
\end{enumerate}
