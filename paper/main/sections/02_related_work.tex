\section{Background and Related Work}
\label{sec:related_work}

This section reviews algorithmic fairness definitions, existing tools, regulatory landscape, and gap analysis that motivates DeepBridge Fairness.

\subsection{Fairness Definitions}

The literature proposes over 20 formal definitions of fairness~\cite{mehrabi2021survey}, organized into three main categories:

\subsubsection{Individual Fairness}

Similar individuals should receive similar treatment~\cite{dwork2012fairness}. Formally, a decision function $f$ satisfies individual fairness if:
\[
d(x_i, x_j) \leq \epsilon \implies d(f(x_i), f(x_j)) \leq \delta
\]
where $d$ is a similarity metric. \textbf{Limitation}: Requires domain-specific similarity metric definition, difficult to specify in practice.

\subsubsection{Group Fairness}

Groups defined by protected attributes should have similar statistical metrics. Main variants:

\textbf{(1) Demographic Parity (Statistical Parity)}~\cite{feldman2015certifying}:
\[
P(\hat{Y}=1 | A=0) = P(\hat{Y}=1 | A=1)
\]
where $A$ is a protected attribute. \textbf{Limitation}: Ignores legitimate differences in base rates.

\textbf{(2) Equalized Odds}~\cite{hardt2016equality}:
\[
P(\hat{Y}=1 | Y=y, A=0) = P(\hat{Y}=1 | Y=y, A=1), \quad \forall y \in \{0,1\}
\]
\textbf{Benefit}: Allows justified differences in base rates, but equalizes error rates.

\textbf{(3) Equal Opportunity}~\cite{hardt2016equality}:
\[
P(\hat{Y}=1 | Y=1, A=0) = P(\hat{Y}=1 | Y=1, A=1)
\]
Variant of equalized odds focusing only on True Positive Rate.

\textbf{(4) Disparate Impact}~\cite{feldman2015certifying}:
\[
\text{DI} = \frac{P(\hat{Y}=1 | A=1)}{P(\hat{Y}=1 | A=0)} \geq 0.80
\]
Based on the EEOC 80\% rule. \textbf{Regulatory connection}: Only metric directly linked to legal requirement.

\subsubsection{Causal Fairness}

Uses causal models to define fairness~\cite{kusner2017counterfactual}. \textbf{Counterfactual Fairness}: A decision $\hat{Y}$ is counterfactually fair if:
\[
P(\hat{Y}_{A \leftarrow a}(U) = y | X=x, A=a) = P(\hat{Y}_{A \leftarrow a'}(U) = y | X=x, A=a)
\]
\textbf{Limitation}: Requires complete knowledge of causal graph, rarely available in practice.

\subsection{Existing Tools}

We review the main open-source tools for fairness analysis:

\subsubsection{AI Fairness 360 (IBM)}

Python framework from IBM with 71 metrics and 11 mitigation algorithms~\cite{bellamy2018ai}.

\textbf{Strengths}:
\begin{itemize}
    \item Broad metric coverage (71 total, but only 8 frequently used)
    \item Pre/in/post-processing mitigation algorithms
    \item Support for multiple bias types (class imbalance, concept drift)
\end{itemize}

\textbf{Limitations}:
\begin{itemize}
    \item \textbf{Custom data format}: Requires conversion to BinaryLabelDataset
    \item \textbf{No regulatory verification}: Does not automatically verify EEOC/ECOA compliance
    \item \textbf{No auto-detection}: User must manually specify protected attributes
    \item \textbf{No threshold optimization}: Does not analyze fairness-accuracy trade-offs
\end{itemize}

\subsubsection{Fairlearn (Microsoft)}

Python toolkit focused on bias mitigation~\cite{bird2020fairlearn}.

\textbf{Strengths}:
\begin{itemize}
    \item Integration with scikit-learn
    \item Mitigation algorithms via constrained optimization (GridSearch, ExponentiatedGradient)
    \item Interactive visualizations (FairlearnDashboard)
\end{itemize}

\textbf{Limitations}:
\begin{itemize}
    \item \textbf{Focus on mitigation vs. detection}: Only 6 detection metrics
    \item \textbf{No pre-training metrics}: Does not analyze bias in training data
    \item \textbf{No regulatory compliance}: Does not verify 80\% rule or Question 21
    \item \textbf{No audit-ready reports}: Interactive visualizations not suitable for auditing
\end{itemize}

\subsubsection{Aequitas (University of Chicago)}

Toolkit focused on public policy and criminal justice~\cite{saleiro2018aequitas}.

\textbf{Strengths}:
\begin{itemize}
    \item User-friendly web interface (no code)
    \item Focus on social justice applications
    \item HTML reports with visualizations
\end{itemize}

\textbf{Limitations}:
\begin{itemize}
    \item \textbf{Only 7 metrics}: Limited coverage (vs. 15 from DeepBridge)
    \item \textbf{No programmatic integration}: Difficult to integrate into CI/CD pipelines
    \item \textbf{No threshold optimization}: Does not recommend optimal threshold
    \item \textbf{No auto-detection}: Requires manual data upload with specified attributes
\end{itemize}

\subsection{Regulatory Landscape}

Fairness regulations impose concrete requirements that tools must meet:

\subsubsection{Equal Employment Opportunity Commission (EEOC) -- United States}

\textbf{80\% Rule}~\cite{eeoc1978uniform}: A selection system has discriminatory impact if:
\[
\text{DI} = \frac{\text{Selection Rate}_{\text{protected}}}{\text{Selection Rate}_{\text{reference}}} < 0.80
\]

\textbf{Question 21 (``Flip-Flop Rule'')}~\cite{eeoc1978uniform}: Groups with representation <2\% lack statistical validity for adverse impact analysis.

\textbf{Gap}: No existing tool automatically verifies both rules.

\subsubsection{Equal Credit Opportunity Act (ECOA) -- United States}

\textbf{Prohibition of discrimination}~\cite{ecoa1974equal}: Creditors cannot discriminate based on race, color, religion, national origin, sex, marital status, age.

\textbf{Adverse Action Notices}: Creditors must provide ``specific reasons'' for adverse decisions (credit denial).

\textbf{Gap}: Existing tools do not automatically generate adverse action notices.

\subsubsection{General Data Protection Regulation (GDPR) -- European Union}

\textbf{Article 22}~\cite{gdpr2016general}: Individuals have the right not to be subject to decisions based solely on automated processing.

\textbf{Right to explanation}: Individuals can request explanation of automated decisions.

\textbf{Gap}: Fairness frameworks focus on statistical metrics, not individual explanations.

\subsection{Gap Analysis: Why DeepBridge Fairness}

Table~\ref{tab:comparison} compares DeepBridge Fairness with existing tools, highlighting filled gaps:

\begin{table}[h]
\centering
\caption{Comparison of fairness tools. DeepBridge is the only one with integrated auto-detection, EEOC/ECOA verification, and threshold optimization.}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Feature} & \textbf{AIF360} & \textbf{Fairlearn} & \textbf{Aequitas} & \textbf{DeepBridge} \\
\midrule
Pre-training metrics & \xmark & \xmark & \xmark & \cmark (4) \\
Post-training metrics & \cmark (8) & \cmark (6) & \cmark (7) & \cmark (11) \\
Auto-detection attributes & \xmark & \xmark & \xmark & \cmark \\
EEOC 80\% verification & \xmark & \xmark & \xmark & \cmark \\
Question 21 verification & \xmark & \xmark & \xmark & \cmark \\
ECOA adverse actions & \xmark & \xmark & \xmark & \cmark \\
Threshold optimization & \xmark & \xmark & \xmark & \cmark \\
Audit-ready reports & \xmark & \xmark & Partial & \cmark \\
Scikit-learn integration & \xmark & \cmark & \xmark & \cmark \\
Interactive visualizations & \xmark & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Main Gaps Filled}:

\begin{enumerate}
    \item \textbf{Research-Regulation Bridge}: DeepBridge is the only tool that automatically verifies EEOC/ECOA requirements, not just academic metrics

    \item \textbf{Complete Automation}: Auto-detection of sensitive attributes eliminates error-prone manual identification (92\% precision, F1 0.90)

    \item \textbf{Complete Coverage}: 15 metrics (4 pre + 11 post) cover 87\% more cases than existing tools

    \item \textbf{Decision Support}: Threshold optimization with Pareto frontier guides deployment (no existing tool offers this)

    \item \textbf{Production-Ready}: PDF/HTML reports approved by compliance officers (100\% approval in 6 organizations)
\end{enumerate}

\subsection{Related Work in ML Systems}

DeepBridge Fairness is inspired by software engineering literature for ML:

\textbf{ML Testing}~\cite{breck2017ml,sculley2015hidden}: Proposes rubrics for production (ML Test Score), but does not specify fairness implementations.

\textbf{Slice-based Analysis}~\cite{chung2019slice,eyuboglu2022domino}: Detects data slices with degraded performance, but does not focus on protected attributes or regulatory compliance.

\textbf{Model Monitoring}~\cite{rabanser2019failing}: Detects drift in production, but does not analyze fairness drift (e.g., disparate impact deteriorating over time).

\textbf{DeepBridge Differential}: First framework that integrates fairness testing into end-to-end validation workflow, with focus on regulatory compliance and production readiness.
