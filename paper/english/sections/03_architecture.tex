\section{DeepBridge Fairness Framework}
\label{sec:architecture}

The DeepBridge Fairness Framework is organized into seven main components that work together to provide automated fairness analysis, regulatory compliance verification, and deployment decision support. This section details each component.

\subsection{Architecture Overview}

The DeepBridge Fairness architecture (Figure~\ref{fig:fairness_architecture}) follows a three-stage pipeline:

\begin{enumerate}
    \item \textbf{Automatic Detection}: Identifies sensitive attributes via fuzzy matching
    \item \textbf{Multi-Dimensional Analysis}: Computes 15 metrics (4 pre-training + 11 post-training)
    \item \textbf{Verification \& Optimization}: Verifies EEOC/ECOA compliance and optimizes thresholds
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{figures/diagram.png}
    \caption{DeepBridge Fairness Framework Architecture showing the three-stage pipeline: automatic sensitive attribute detection, multi-dimensional fairness analysis with 15 metrics, and EEOC/ECOA compliance verification with threshold optimization.}
    \Description{A diagram showing the DeepBridge Fairness architecture with three main stages: input data flowing through automatic detection, then multi-dimensional analysis computing 15 fairness metrics, and finally verification and optimization producing compliance reports and optimal thresholds.}
    \label{fig:fairness_architecture}
\end{figure}

\begin{figure}[!htbp]
\begin{lstlisting}[language=Python, caption=Complete DeepBridge Fairness workflow]
from deepbridge import DBDataset, FairnessTestManager

# Stage 1: Create dataset with auto-detection
dataset = DBDataset(
    data=df,
    target_column='approved',
    model=trained_model
)
# Detected attributes: ['gender', 'race', 'age']

# Stage 2: Multi-dimensional analysis
ftm = FairnessTestManager(dataset)
results = ftm.run_all_tests()
# 15 metrics automatically computed

# Stage 3: EEOC/ECOA verification + optimization
compliance = ftm.check_eeoc_compliance()
optimal_threshold = ftm.optimize_threshold(
    fairness_metric='disparate_impact',
    min_accuracy=0.80
)
\end{lstlisting}
\end{figure}

\subsection{Auto-Detection of Sensitive Attributes}

\subsubsection{Fuzzy Matching Algorithm}

DeepBridge uses fuzzy string matching to automatically detect sensitive attributes in column names, eliminating manual specification.

\textbf{Protected Attribute Categories}: EEOC and ECOA define 7 categories:
\begin{enumerate}
    \item \textbf{Gender}: gender, sex, female, male, gender\_identity
    \item \textbf{Race}: race, ethnicity, african\_american, hispanic, asian, white
    \item \textbf{Age}: age, dob, date\_of\_birth, birth\_year, yob
    \item \textbf{Religion}: religion, faith, religious\_affiliation
    \item \textbf{Disability}: disability, handicap, disabled, impairment
    \item \textbf{Nationality}: nationality, country\_of\_birth, citizenship, national\_origin
    \item \textbf{Marital Status}: marital\_status, married, single, divorced
\end{enumerate}

\textbf{Algorithm}:
\begin{algorithm}
\caption{Auto-Detection of Sensitive Attributes}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$ with features $F = \{f_1, ..., f_n\}$
\REQUIRE Keyword dictionary $K$ by category
\REQUIRE Similarity threshold $\theta$ (default: 0.85)
\ENSURE Set $S$ of detected sensitive attributes
\STATE $S \leftarrow \emptyset$
\FOR{each feature $f_i \in F$}
    \STATE $f_{\text{clean}} \leftarrow$ normalize($f_i$) // lowercase, remove underscores
    \FOR{each category $c \in K$}
        \FOR{each keyword $k \in K[c]$}
            \STATE $\text{sim} \leftarrow$ Levenshtein\_similarity($f_{\text{clean}}$, $k$)
            \IF{$\text{sim} \geq \theta$}
                \STATE $S \leftarrow S \cup \{(f_i, c, \text{sim})\}$
            \ENDIF
        \ENDFOR
    \ENDFOR
\ENDFOR
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\textbf{Threshold Calibration}: Threshold $\theta=0.85$ was calibrated on 500 real datasets to maximize F1-score:
\begin{itemize}
    \item \textbf{Precision}: 92\% (low false positive rate)
    \item \textbf{Recall}: 89\% (detects most attributes)
    \item \textbf{F1-Score}: 0.90
\end{itemize}

\textbf{Manual Override}: Users can override automatic detection:
\begin{lstlisting}[language=Python]
# Accept automatic detection
dataset.protected_attributes = dataset.detected_sensitive_attributes

# Or manual override
dataset.protected_attributes = ['gender', 'race']
\end{lstlisting}

\subsection{Fairness Metrics Suite}

\subsubsection{Pre-Training Metrics (4)}

Analyze bias in \textit{training data} before training model:

\textbf{(1) Class Balance}:
\[
\text{CB}(A) = \min_{a \in A} \frac{P(Y=1|A=a)}{\max_{a' \in A} P(Y=1|A=a')}
\]
Detects imbalance in positive label rates between groups. Threshold: CB < 0.80 indicates bias.

\textbf{(2) Concept Balance}:
\[
\text{ConceptB}(A) = \frac{\text{H}(Y|A)}{\text{H}(Y)}
\]
where H is entropy. Measures if protected attribute is predictive of label (redundancy).

\textbf{(3-4) KL and JS Divergence}:
\[
\text{KL}(P_{A=0}(X) || P_{A=1}(X)), \quad \text{JS}(P_{A=0}(X), P_{A=1}(X))
\]
Measure difference in feature distribution between protected groups.

\textbf{Practical Use}: Pre-training metrics guide mitigation strategies (resampling, reweighting) \textit{before} training expensive models.

\subsubsection{Post-Training Metrics (11)}

Analyze bias in \textit{model predictions} after training:

\textbf{(1) Statistical Parity (Demographic Parity)}:
\[
\text{SP} = P(\hat{Y}=1|A=1) - P(\hat{Y}=1|A=0)
\]
Ideal: $|\text{SP}| < 0.1$ (10pp difference).

\textbf{(2) Disparate Impact}:
\[
\text{DI} = \frac{P(\hat{Y}=1|A=1)}{P(\hat{Y}=1|A=0)}
\]
\textbf{EEOC connection}: DI < 0.80 violates 80\% rule.

\textbf{(3) Equal Opportunity}:
\[
\text{EO} = P(\hat{Y}=1|Y=1, A=1) - P(\hat{Y}=1|Y=1, A=0)
\]
Equalizes True Positive Rates. Ideal: $|\text{EO}| < 0.1$.

\textbf{(4) Equalized Odds}:
\[
\text{EOdds} = \max(|\text{TPR}_{A=1} - \text{TPR}_{A=0}|, |\text{FPR}_{A=1} - \text{FPR}_{A=0}|)
\]
Equalizes TPR \textit{and} FPR. Ideal: EOdds < 0.1.

\textbf{(5) FNR Difference}:
\[
\Delta \text{FNR} = \text{FNR}_{A=1} - \text{FNR}_{A=0}
\]
Detects bias in False Negative errors (e.g., denying credit to qualified candidates).

\textbf{(6-7) Conditional Acceptance/Rejection Parity}:
\[
P(Y=1|\hat{Y}=1, A=1) = P(Y=1|\hat{Y}=1, A=0)
\]
Precision parity: among positive predictions, same rate of true positives.

\textbf{(8-9) Precision/Accuracy Difference}:
\[
\Delta \text{Prec} = \text{Prec}_{A=1} - \text{Prec}_{A=0}, \quad \Delta \text{Acc} = \text{Acc}_{A=1} - \text{Acc}_{A=0}
\]

\textbf{(10) Treatment Equality}:
\[
\text{TE} = \frac{\text{FN}_{A=1}}{\text{FP}_{A=1}} - \frac{\text{FN}_{A=0}}{\text{FP}_{A=0}}
\]
Error ratio (FN/FP) should be equal between groups.

\textbf{(11) Entropy Index}:
\[
\text{EI} = \sum_{a \in A} P(A=a) \cdot \text{H}(\hat{Y}|A=a)
\]
Measures heterogeneity of predictions within groups.

\subsection{EEOC Compliance Verification Module}

\subsubsection{80\% Rule (Disparate Impact)}

Automatically verifies if $\text{DI} \geq 0.80$:

\begin{figure}[!htbp]
\begin{lstlisting}[language=Python, caption=Automatic 80\% rule verification]
def check_80_rule(y_pred, sensitive_attr):
    groups = sensitive_attr.unique()
    selection_rates = {}

    for group in groups:
        mask = (sensitive_attr == group)
        selection_rates[group] = y_pred[mask].mean()

    reference = max(selection_rates.values())
    violations = {}

    for group, rate in selection_rates.items():
        di = rate / reference
        if di < 0.80:
            violations[group] = {
                'DI': di,
                'selection_rate': rate,
                'reference_rate': reference,
                'shortfall': 0.80 - di
            }

    return {
        'compliant': len(violations) == 0,
        'violations': violations
    }
\end{lstlisting}
\end{figure}

\textbf{Generated Report}:
\begin{verbatim}
EEOC 80% Rule Verification:
- Female: DI = 0.72 [VIOLATION] (shortfall: 8pp)
- Male: DI = 1.00 [COMPLIANT]
Recommendation: Adjust threshold or retrain model
\end{verbatim}

\subsubsection{Question 21 (Minimum 2\% Representation)}

EEOC Question 21 stipulates that groups with <2\% representation lack statistical validity:

\begin{figure}[!htbp]
\begin{lstlisting}[language=Python, caption=Question 21 verification]
def check_question_21(sensitive_attr, min_representation=0.02):
    total = len(sensitive_attr)
    warnings = {}

    for group in sensitive_attr.unique():
        count = (sensitive_attr == group).sum()
        representation = count / total

        if representation < min_representation:
            warnings[group] = {
                'count': count,
                'representation': representation,
                'required': min_representation,
                'warning': 'Insufficient sample size for statistical validity'
            }

    return {
        'valid': len(warnings) == 0,
        'warnings': warnings
    }
\end{lstlisting}
\end{figure}

\textbf{Automatic Action}: Groups with <2\% are excluded from disparate impact analysis, avoiding false positives.

\subsection{Threshold Optimization}

\subsubsection{Fairness-Accuracy Trade-off Analysis}

DeepBridge analyzes threshold range (10-90\%) and computes fairness and accuracy metrics for each threshold:

\begin{figure}[!htbp]
\begin{lstlisting}[language=Python, caption=Multi-objective threshold optimization]
from deepbridge import FairnessTestManager

ftm = FairnessTestManager(dataset)

# Trade-off analysis in range 0.1-0.9
threshold_analysis = ftm.analyze_thresholds(
    thresholds=np.arange(0.1, 0.9, 0.05),
    fairness_metrics=['disparate_impact', 'equal_opportunity'],
    performance_metrics=['accuracy', 'f1_score']
)

# Pareto frontier: non-dominated thresholds
pareto_thresholds = threshold_analysis['pareto_frontier']

# Recommendation based on constraints
optimal = ftm.recommend_threshold(
    min_disparate_impact=0.80,
    min_accuracy=0.75,
    objective='maximize_f1'
)
\end{lstlisting}
\end{figure}

\subsubsection{Pareto Frontier}

Threshold $t_1$ dominates $t_2$ if:
\begin{itemize}
    \item $\text{DI}(t_1) \geq \text{DI}(t_2)$ (better fairness)
    \item $\text{Acc}(t_1) \geq \text{Acc}(t_2)$ (better accuracy)
    \item At least one inequality is strict
\end{itemize}

Pareto frontier contains non-dominated thresholds, allowing stakeholders to choose appropriate trade-off.

\subsection{Statistical Representativeness}

DeepBridge implements representativeness validations to avoid spurious conclusions:

\textbf{(1) Minimum Group Size}: Groups with n < 30 receive warning (statistical rule of thumb).

\textbf{(2) Confidence Intervals}: Metrics reported with 95\% CI using bootstrap:
\begin{lstlisting}[language=Python]
def compute_with_ci(metric_fn, y_true, y_pred, n_bootstrap=1000):
    bootstrap_scores = []
    n = len(y_true)

    for _ in range(n_bootstrap):
        indices = np.random.choice(n, n, replace=True)
        score = metric_fn(y_true[indices], y_pred[indices])
        bootstrap_scores.append(score)

    return {
        'mean': np.mean(bootstrap_scores),
        'ci_lower': np.percentile(bootstrap_scores, 2.5),
        'ci_upper': np.percentile(bootstrap_scores, 97.5)
    }
\end{lstlisting}

\textbf{(3) Significance Tests}: Differences between groups tested via permutation test (p-value < 0.05).

\subsection{Visualization System}

DeepBridge automatically generates 6 visualization types:

\textbf{(1) Distribution by Group}: Histograms of features by protected group

\textbf{(2) Metrics Comparison}: Barplot comparing 15 metrics between groups

\textbf{(3) Threshold Impact Analysis}: Curves showing how metrics vary with threshold

\textbf{(4) Confusion Matrices per Group}: Side-by-side confusion matrices for each group

\textbf{(5) Fairness Radar Chart}: Radar chart with 11 normalized post-training metrics

\textbf{(6) Group Performance Comparison}: Boxplots of performance metrics (accuracy, precision, recall, F1) by group

\textbf{Report Formats}:
\begin{itemize}
    \item \textbf{Interactive HTML}: Plotly charts, dynamic filters
    \item \textbf{Static HTML}: For auditing (attachable to emails)
    \item \textbf{PDF}: Corporate format with customizable branding
    \item \textbf{JSON}: For programmatic integration
\end{itemize}

\subsection{Integration with DeepBridge Validation Pipeline}

FairnessTestManager integrates with DeepBridge's Experiment orchestrator:

\begin{lstlisting}[language=Python, caption=Integration with complete pipeline]
from deepbridge import DBDataset, Experiment

dataset = DBDataset(df, target='approved', model=model)

# Multi-dimensional validation (fairness + robustness + uncertainty)
exp = Experiment(
    dataset=dataset,
    tests=['fairness', 'robustness', 'uncertainty']
)

results = exp.run_tests()

# Unified report with all dimensions
exp.save_pdf('complete_validation_report.pdf')
\end{lstlisting}

\textbf{Integration Benefits}:
\begin{itemize}
    \item \textbf{Consistency}: Same DBDataset used across fairness, robustness, uncertainty
    \item \textbf{Efficiency}: Model predictions computed once and reused
    \item \textbf{Unified Reports}: Stakeholders see fairness in context of other validation dimensions
\end{itemize}
